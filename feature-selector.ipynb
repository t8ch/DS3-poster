{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, optimizers, models, Sequential, initializers, constraints, regularizers, backend\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from sklearn import datasets, linear_model, preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook',font_scale=1.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel contraint that effectively implements one-to-one connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define feature selection layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# all you need to create a mask matrix M, which is a NxN identity matrix\n",
    "# and you can write a contraint like below\n",
    "class DiagonalWeight(constraints.Constraint):\n",
    "    \"\"\"Constrains the weights to be diagonal.\n",
    "    \"\"\"\n",
    "    def __call__(self, w):\n",
    "        N = tf.shape(w)[-1]\n",
    "        m = tf.eye(N)\n",
    "        w = m*w\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regularizer: L1 + time-dependent tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modified L0 loss: $\\tilde L_0 = \\alpha \\sum_n |w_n+\\beta|$, with $\\beta=0.05$ here.\n",
    "\n",
    "only one-to-one connections (=diagonal weight matrix): $w_{nk} = w_{n}\\text{ if } $n=k$\\text{ and 0 otherwise}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class L1_tilde(regularizers.Regularizer):\n",
    "    \"\"\"Regularizer for ...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalization = 1000, alpha = .05):\n",
    "        self.counter = 0\n",
    "        self.normalization = normalization\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, x):\n",
    "        regularization = 0.\n",
    "        prefactor = min(1,self.counter/self.normalization)\n",
    "        regularization += backend.sum(backend.abs(x+0.05))\n",
    "        self.counter += 1\n",
    "        return self.alpha*regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class selection_layer(layers.Dense):\n",
    "    def __init__(self, units, norm=1000, alpha=0.05):\n",
    "        super(selection_layer, self).__init__(units, kernel_constraint=DiagonalWeight(),\n",
    "                                        kernel_initializer = initializers.Ones(),\n",
    "                                        kernel_regularizer= L1_tilde(alpha=alpha, normalization=norm),\n",
    "                                        #bias_regularizer=constant_bias(),\n",
    "                                        activation='relu',\n",
    "                                        use_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        weights = tf.linalg.tensor_diag_part((model.layers[0].weights[0]).numpy())\n",
    "        weights_history.append(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## most naive test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([selection_layer(5)])\n",
    "model.compile(optimizer='rmsprop', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_x = np.random.normal(size=(600,5))\n",
    "test_y = np.concatenate((test_x[:,:4],np.random.normal(size = (600,1))), axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.fit(test_x, test_y, epochs = 300, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### train for different alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weights = np.array([])\n",
    "r2s = np.array([])\n",
    "for al in np.linspace(0.001, 2, 5):\n",
    "    model = Sequential([selection_layer(units=X.shape[-1], alpha = al, norm=10000), layers.Dense(5, activation='relu'), layers.Dense(1)])\n",
    "    model.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "    model.fit(X_train, y_train, epochs=500)\n",
    "    weights = np.append(weights,tf.linalg.tensor_diag_part(model.layers[0].weights[0]).numpy())\n",
    "    r2s = np.append(r2s,r2_score(y_test,model.predict(X_test)))\n",
    "weights = weights.reshape(-1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#plt.figure()\n",
    "ax = sns.heatmap((weights>0), cmap='Blues', cbar=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## classification data (sklearn dummy data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples = 1200, n_informative=10,n_repeated=0, n_classes=3, class_sep=1., n_clusters_per_class=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:, 1], X[:, 3],marker='o', c=y, s=25, edgecolor='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#y = (y-y.mean())/y.std()\n",
    "minmax = preprocessing.MinMaxScaler(feature_range=(0,10))\n",
    "minmax.fit(X)\n",
    "X = minmax.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weights_history = []\n",
    "loss = []\n",
    "val_loss = []\n",
    "\n",
    "num_alphas = 20\n",
    "alpha_range = np.logspace(-2.4, 0.1, num_alphas)\n",
    "\n",
    "weights = np.array([])\n",
    "acc = np.array([])\n",
    "for al in alpha_range:\n",
    "    model = Sequential([selection_layer(units=X.shape[-1], alpha = al, norm=.5*100*int(800/32.)), layers.Dense(10, activation='relu'), layers.Dropout(.1), layers.Dense(3, activation='softmax')])\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "    hi = model.fit(X_train, tf.keras.utils.to_categorical(y_train), epochs=340, validation_split = 0.15, callbacks=[MyCallback()])\n",
    "    weights = np.append(weights,tf.linalg.tensor_diag_part(model.layers[0].weights[0]).numpy())\n",
    "    acc = np.append(acc, accuracy_score(y_test,model.predict_classes(X_test)))\n",
    "    loss= np.append(loss, hi.history['loss'])\n",
    "    val_loss = np.append(val_loss, hi.history['val_loss'])\n",
    "weights = weights.reshape(-1,20)\n",
    "weights_history = np.array(weights_history).reshape(num_alphas,-1,X.shape[-1])\n",
    "loss = loss.reshape(num_alphas,-1)\n",
    "val_loss = val_loss.reshape(num_alphas,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.savez('classifier_for_poster', weight=weights, weights_history=weights_history, alpha_range=alpha_range, loss=loss, val_loss=val_loss, acc=acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.load('classifier_for_poster.npz')\n",
    "weights, weights_history, alpha_range, loss, val_loss = data.f.weight, data.f.weights_history, data.f.alpha_range, data.f.loss, data.f.val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "plot performance and selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(6,8))\n",
    "#fig.subplots_adjust(hspace=0.5)\n",
    "fig.suptitle(r'Performance and feature selection as function of $\\alpha$')\n",
    "\n",
    "ax[0].plot(alpha_range, acc, color = 'olivedrab')\n",
    "#xlabel(r'regularization strength $\\alpha$')\n",
    "ax[0].set_ylabel(r'accuracy score')\n",
    "ax[0].set_xscale('log')\n",
    "\n",
    "tick = ticker.ScalarFormatter(useOffset=False, useMathText=True)\n",
    "tick.set_powerlimits((0,0))\n",
    "tg = [u\"${}$\".format(tick.format_data(round(x,3))) for x in alpha_range]\n",
    "\n",
    "sns.heatmap((weights.T>0), ax = ax[1], cmap='tab20c_r', cbar=False, alpha=.7, linewidth=.5, xticklabels=tg)\n",
    "ax[1].set_ylabel('feature #')\n",
    "ax[1].set_xlabel(r'regularization strength $\\alpha$')\n",
    "ax[1].axhline(y=10, color = 'k', ls = ':')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ind = -5\n",
    "[plt.plot(x, c = 'indianred', lw = .75, alpha = .7) for x in weights_history[ind].T[:9]]\n",
    "plt.plot(weights_history[ind].T[9], c = 'indianred', lw = .75, alpha = .7, label='informative features')\n",
    "[plt.plot(x, c = 'dodgerblue', lw = .75, alpha = .7) for x in weights_history[ind].T[11:]]\n",
    "plt.plot(weights_history[ind].T[10], c = 'dodgerblue', lw = .75, alpha = .7, label='non-informative features')\n",
    "plt.xlabel('# batches')\n",
    "plt.ylabel('selection layer weights')\n",
    "plt.title(r'$\\alpha$={:.2f}'.format(alpha_range[ind]))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ind = -5\n",
    "plt.figure()\n",
    "plt.plot(loss[ind], label='loss')\n",
    "plt.plot(val_loss[ind], label='validation loss')\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('losses')\n",
    "plt.title(r'$\\alpha$={:.2f}'.format(alpha_range[ind]))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### single $\\alpha$ run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights_history = []\n",
    "model = Sequential([selection_layer(units=X.shape[-1], alpha = 2., norm=100*int(800/32.)), layers.Dense(10, activation='relu'), layers.Dropout(.1), layers.Dense(3, activation='softmax')])\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "history = model.fit(X_train, tf.keras.utils.to_categorical(y_train), epochs=14, validation_split = 0.15, callbacks=[MyCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hippocampus data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%pylab\n",
    "\n",
    "from scipy.io import loadmat\n",
    "sys.path.append('/home/herfurtht/mpi-br/project1/')\n",
    "sys.path.append('/home/herfurtht/mpi-br/rat/Neural_Decoding_fork/')\n",
    "\n",
    "import preprocessing_funcs\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mat = loadmat('hippo/data_CA1.mat')  # load mat-file\n",
    "mat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mat['time'] = arange(len(mat['run_speed']))/1000. #time in sec\n",
    "mat['run_speed'], mat['x_pos'], mat['y_pos'] = mat['run_speed'].ravel(), mat['x_pos'].ravel(), mat['y_pos'].ravel()\n",
    "mat['spikes'].shape, mat['x_pos'].reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time_shift = 0 #here in ms\n",
    "\n",
    "mat['spikes'] = mat['spikes'][:]\n",
    "spike_times = preprocessing_funcs.binary_to_times(mat['spikes'], .001)\n",
    "t_start = 0.\n",
    "t_end = 595.- time_shift/1000.\n",
    "vel_times = arange(0, 595., .001)\n",
    "vels = array(list(zip(mat['x_pos'], mat['y_pos'])))\n",
    "\n",
    "vels = vels[time_shift:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "figure()\n",
    "times = 0, 595000\n",
    "scatter(mat['x_pos'][times[0]:times[1]], mat['y_pos'][times[0]:times[1]], c = arange(len(mat['x_pos'][times[0]:times[1]])), norm = mpl.colors.Normalize(vmin=0., vmax= len(mat['x_pos'][times[0]:times[1]])), cmap = cm.jet, s = .3)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dt= .05 #Size of time bins (in seconds)\n",
    "downsample_factor=1 #Downsampling of output (to make binning go faster). 1 means no downsamplinga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "###Preprocessing to put spikes and output in bins###\n",
    "\n",
    "#Bin neural data using \"bin_spikes\" function\n",
    "neural_data= preprocessing_funcs.bin_spikes(spike_times,dt,t_start,t_end)\n",
    "### remove neurons with too little spikes\n",
    "neural_data = neural_data[:, neural_data.sum(0)> 10]\n",
    "\n",
    "#Bin output (velocity) data using \"bin_output\" function\n",
    "vels_binned= preprocessing_funcs.bin_output(vels,vel_times,dt,t_start,t_end,downsample_factor)\n",
    "\n",
    "#velocities in either direction\n",
    "#vels_binned = gradient(vels_binned, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#6, 1, 6 before 0, 1, 21\n",
    "bins_before= 5 #How many bins of neural data prior to the output are used for decoding\n",
    "bins_current = 1 #Whether to use concurrent time bin of neural data\n",
    "bins_after= 5 #How many bins of neural data after the output are used for decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Format for recurrent neural networks (SimpleRNN, GRU, LSTM)\n",
    "# Function to get the covariate matrix that includes spike history from previous bins\n",
    "X=preprocessing_funcs.get_spikes_with_history(neural_data,bins_before,bins_after,bins_current)\n",
    "\n",
    "# Format for Wiener Filter, Wiener Cascade, XGBoost, and Dense Neural Network\n",
    "#Put in \"flat\" format, so each \"neuron / time\" is a single feature\n",
    "X_flat=X.reshape(X.shape[0],(X.shape[1]*X.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Set decoding output\n",
    "y=vels_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_mean, X_train, X_test, X_valid, X_flat_train_mean, X_flat_train, X_flat_test, X_flat_valid, y_train_mean, y_train, y_test, y_valid = preprocessing_funcs.get_training_data(X,y, [.8,.8], bins_before, bins_after)\n",
    "X_test, y_test = X_valid, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## build decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, SimpleRNN, GRU, Activation, Dropout, Conv1D, concatenate, Flatten, TimeDistributed\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " #Declare model\n",
    "model=Sequential() #Declare model\n",
    "# Add selection layer (Time distributed)\n",
    "model.add(TimeDistributed(selection_layer(units=X_train.shape[2], alpha = 5000, norm=100)))\n",
    "\n",
    "#Add recurrent layer\n",
    "model.add(LSTM(64, recurrent_dropout=.1,dropout=.1)) #Within recurrent layer, include dropout\n",
    "model.add(Dropout(.1)) #Dropout some units (recurrent layer output units)\n",
    "\n",
    "#Add dense connections to output layer\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "#Fit model (and set fitting parameters)\n",
    "model.compile(loss='mse',optimizer='rmsprop',metrics=['accuracy']) #Set loss function and optimizer\n",
    "#Fit the model\n",
    "model.fit(X_train,y_train, epochs=10,verbose=1, validation_split = .15, callbacks = [EarlyStopping(monitor='val_loss', min_delta=0, patience= 2, verbose=0, mode='auto'), MyCallback()]) #Get predictions\n",
    "y_valid_predicted_lstm=model.predict(X_valid)\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_lstm=r2_score(y_valid,y_valid_predicted_lstm)\n",
    "\n",
    "print('R2s:', R2s_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "weights_history = []\n",
    "loss = []\n",
    "val_loss = []\n",
    "\n",
    "num_alphas = 12\n",
    "alpha_range = np.logspace(-.3, 3.1, num_alphas)\n",
    "\n",
    "weights = np.array([])\n",
    "acc = np.array([])\n",
    "for al in alpha_range:\n",
    "    model=Sequential() #Declare model\n",
    "    # Add selection layer (Time distributed)\n",
    "    model.add(TimeDistributed(selection_layer(units=X_train.shape[2], alpha = al, norm=100)))\n",
    "\n",
    "    #Add recurrent layer\n",
    "    model.add(LSTM(64, recurrent_dropout=.1,dropout=.1)) #Within recurrent layer, include dropout\n",
    "    model.add(Dropout(.1)) #Dropout some units (recurrent layer output units)\n",
    "\n",
    "    #Add dense connections to output layer\n",
    "    model.add(Dense(y_train.shape[1]))\n",
    "    #Fit model (and set fitting parameters)\n",
    "    model.compile(loss='mse',optimizer='rmsprop',metrics=['accuracy']) #Set loss function and optimizer\n",
    "    #Fit the model\n",
    "    hi = model.fit(X_train,y_train, epochs=25, verbose=1, validation_split = .15, \n",
    "                   callbacks = [EarlyStopping(monitor='val_loss', min_delta=0, patience= 10, verbose=0, mode='auto'), MyCallback()]) \n",
    "    weights = np.append(weights,tf.linalg.tensor_diag_part(model.layers[0].weights[0]).numpy())\n",
    "    acc = np.append(acc, r2_score(y_test,model.predict(X_test)))\n",
    "    loss= np.append(loss, hi.history['loss'])\n",
    "    val_loss = np.append(val_loss, hi.history['val_loss'])\n",
    "weights = weights.reshape(-1,X_train.shape[2])\n",
    "weights_history = np.array(weights_history).reshape(num_alphas,-1, X_train.shape[2])\n",
    "loss = loss.reshape(num_alphas,-1)\n",
    "val_loss = val_loss.reshape(num_alphas,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "save for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.savez('hippocampus_for_poster', weights=weights, weights_history=weights_history, alpha_range=alpha_range, loss=loss, val_loss=val_loss, acc=acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.load('hippocampus_for_poster.npz')\n",
    "weights, weights_history, alpha_range, loss, val_loss = data.f.weight, data.f.weights_history, data.f.alpha_range, data.f.loss, data.f.val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ind = -2\n",
    "[plt.plot(x, c = 'indianred', lw = .5) for x in weights_history[ind].T[:]]\n",
    "#[plt.plot(x, c = 'dodgerblue') for x in weights_history[ind].T[10:]]\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "performance and selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "alpha_new = array(['{:.2f}'.format(x) for x in alpha_range])\n",
    "alpha_new = alpha_new.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(6,8))\n",
    "#fig.subplots_adjust(hspace=0.5)\n",
    "fig.suptitle(r'Performance and feature selection as function of $\\alpha$')\n",
    "\n",
    "ax[0].plot(alpha_range, acc, color = 'olivedrab')\n",
    "#xlabel(r'regularization strength $\\alpha$')\n",
    "ax[0].set_ylabel(r'$r^2$-score')\n",
    "ax[0].set_xscale('log')\n",
    "\n",
    "tick = ticker.ScalarFormatter(useOffset=False, useMathText=True)\n",
    "tick.set_powerlimits((0,0))\n",
    "tg = [u\"${}$\".format(tick.format_data(x)) for x in alpha_new]\n",
    "\n",
    "sns.heatmap((weights[:,argsort(sum(weights>0, axis=0))].T>0), ax=ax[1],cmap='tab20c_r', alpha=.7, cbar=False, linewidth=.3, linecolor = 'k', xticklabels=tg)\n",
    "ax[1].set_ylabel('cell # (sorted)')\n",
    "ax[1].set_xlabel(r'regularization strength $\\alpha$')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ind = -2\n",
    "[plt.plot(x, c = 'dodgerblue', lw = .75, alpha = .7) for x in weights_history[ind].T]\n",
    "plt.xlabel('# batches')\n",
    "plt.ylabel('selection layer weights')\n",
    "plt.title(r'$\\alpha$={:.2f}'.format(alpha_range[ind]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ind = -2\n",
    "plt.figure()\n",
    "plt.plot(loss[ind], label='loss')\n",
    "plt.plot(val_loss[ind], label='validation loss')\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('losses')\n",
    "plt.title(r'$\\alpha$={:.2f}'.format(alpha_range[ind]))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**notes**\n",
    "- schedulder for offset (!)\n",
    "- compare different feature selectors (and performance at random set with same size)\n",
    "- show correlations\n",
    "- hippocampus temporal data\n",
    "- better for removing unimportant than for ranking!\n",
    "- only for positive values"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.3",
    "jupytext_version": "0.8.6"
   }
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "276px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
